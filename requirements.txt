For the experiment:

Create instructions for participants:
-Structure of the experiment
	First: Eye-tracker calibration
	Second: Robot/4K Calibration
	Third: Experiment
-Left and right arrow with keyboard
-Starting and stopping eye-tracker

Make square for participants to stand in.
Make clear which is the right and left.


For the robot:
Delete VU logo (also before the start of the experiment)



Done:
Fix the 'calibration' output during experiment
visual to tablet speech (replace the 'focus on visual' to 'focus on tablet')
Add the yaw head fix.
Implement training part more properly
Tell participants when first trial was over. 

Confirm implemented:
only in training: give user feedback on clicking the wrong key and on missing a keystroke at all. 

Current:
Inverse the left and right gesture (because the tablet has the direct output). 

===============

Fix filename.
Note of order: with and without eye-tracker. Also add to filenames?


Clean stopped? and key was pressed from the output. Perhaps replace it with the keypressed and the correctness of the output.
Confirm if it saved the focus_times and data_frames.json properly.